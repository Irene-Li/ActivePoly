{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6aaebd-cc0e-4a13-8a22-75a447da737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 13:48:49.825758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fda98d-5210-433f-879b-6ad37ed4588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8025703545183407 0.9944207885383278\n"
     ]
    }
   ],
   "source": [
    "# Generate some synthetic data\n",
    "true_mean = 2.0\n",
    "true_stddev = 1.0\n",
    "Y = np.random.normal(loc=true_mean, scale=true_stddev, size=100)\n",
    "print(np.mean(Y), np.std(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b61ca30-2271-4ab1-a3f3-24c248ce8db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8062607  0.99680305]\n"
     ]
    }
   ],
   "source": [
    "# Define the log-likelihood function\n",
    "def log_likelihood(params):\n",
    "    mu, sigma = params[0], params[1]\n",
    "    likelihoods = tfp.distributions.Normal(loc=mu, scale=sigma).log_prob(Y)\n",
    "    return tf.reduce_sum(likelihoods)\n",
    "\n",
    "# Define initial parameter values\n",
    "params_initial = tf.Variable([0.0, 2.0])\n",
    "\n",
    "# Use TensorFlow's GradientTape to compute gradients\n",
    "optimizer = tf.optimizers.Adam(0.5)\n",
    "\n",
    "for step in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = -log_likelihood(params_initial)\n",
    "    gradients = tape.gradient(loss, [params_initial])\n",
    "    optimizer.apply_gradients(zip(gradients, [params_initial]))\n",
    "\n",
    "print(params_initial.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b997c82-5a36-4288-98b5-eaa071d3ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Information Matrix:\n",
      "[[9.9364379e-03 3.7051737e-05]\n",
      " [3.7051883e-05 5.0039501e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the Fisher Information Matrix using the Hessian\n",
    "with tf.GradientTape() as tape:\n",
    "    with tf.GradientTape() as tape2:\n",
    "        loss = -log_likelihood(params_initial)  # Remove the extra minus sign here\n",
    "    gradients = tape2.gradient(loss, params_initial)\n",
    "hessian = tape.jacobian(gradients, params_initial)\n",
    "fim = tf.linalg.inv(hessian)\n",
    "print(\"Fisher Information Matrix:\")\n",
    "print(fim.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f97c293-768a-4da2-b403-b0cc4bcb4fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagonals = tf.Variable([1, 1, 1])\n",
    "tf.linalg.diag(diagonals).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e58165f-7a4d-4a53-ba7c-2100adffdc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([0, 1])\n",
    "\n",
    "tf.gather(v, [0, 1, 1, 1]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7fc466-f10d-4a43-b077-209487469498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_params(params): \n",
    "    ''' \n",
    "    lambda, gamma, mu, c, b, sigma_d, sigma_p \n",
    "    '''\n",
    "    c = params[3] \n",
    "    J = tf.gather(params, [0, 2, 0, 1])\n",
    "    J = tf.tensor_scatter_nd_update(J, [[2]], [J[2]*c])\n",
    "    J = tf.reshape(J, (2, 2))\n",
    "    B = tf.linalg.diag(params[5:]**2)\n",
    "    B = tf.tensor_scatter_nd_update(B, [[1, 1]], [B[1, 1] + B[0, 0]*c*c])\n",
    "    B = tf.tensor_scatter_nd_update(B, [[1, 0]], [B[0, 0]*c])\n",
    "    B = tf.tensor_scatter_nd_update(B, [[0, 1]], [B[0, 1]])\n",
    "    return J, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557099c1-b5d2-482f-b97f-93dfe0fdd254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  3.]\n",
      " [-4. -2.]] [[  64.    0.]\n",
      " [ 256. 1105.]]\n"
     ]
    }
   ],
   "source": [
    "params = tf.Variable([-1, -2, 3, 4, 5, 8, 9], dtype=tf.double)\n",
    "J, B  = convert_params(params)\n",
    "print(J.numpy(), B.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0761c08-46fb-48c0-8b98-3d6de32e5c70",
   "metadata": {},
   "source": [
    "## test out multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a68569-2830-4512-9af6-da4013bbc044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.86926927, -6.86926927, -6.86926927, -6.86926927, -6.86926927,\n",
       "       -6.86926927, -6.86926927, -6.86926927, -6.86926927, -6.86926927])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((10, 2))\n",
    "loc = x @ tf.transpose(J) \n",
    "\n",
    "dist = tfp.distributions.MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(B))\n",
    "dist.log_prob(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53cb65fe-130f-4a56-9e5e-7fd077ed3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minuslogP(params, traj, dt): \n",
    "    J, B = convert_params(params) \n",
    "    b = params[4]\n",
    "    tril = tf.linalg.cholesky(B*dt)\n",
    "    dx = traj[:, 1:] - traj[:, :-1]\n",
    "    det = J @ traj[:, :-1]\n",
    "    det = tf.tensor_scatter_nd_update(det, [[1]], [det[1] - b * det[0]**3])\n",
    "    dist = tfp.distributions.MultivariateNormalTriL(loc=tf.transpose(det)*dt, scale_tril=tril)\n",
    "    return  - tf.reduce_sum(dist.log_prob(dx.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfcf9792-5e35-4856-b5e6-62a1b55a13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.030888668828595\n"
     ]
    }
   ],
   "source": [
    "params = tf.Variable([-1, -2, 3, 4, 5, 8, 9], dtype=tf.double)\n",
    "\n",
    "traj = np.zeros((2, 10))\n",
    "print(minuslogP(params, traj, 1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "518b400e-e88b-4921-8948-cba22d822cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 3., 4., 5., 8., 9.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.maximum(0.0, params).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b8efc56-93c3-4113-988a-9845eface45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function evaluations: 5\n"
     ]
    }
   ],
   "source": [
    "minimum = np.array([1.0, 1.0])  # The center of the quadratic bowl.\n",
    "scales = np.array([2.0, 3.0])  # The scales along the two axes.\n",
    "\n",
    "# The objective function and the gradient.\n",
    "cost = lambda x: tf.reduce_sum(scales * tf.math.squared_difference(x, minimum), axis=-1)\n",
    "def quadratic_loss_and_gradient(x):\n",
    "    return tfp.math.value_and_gradient(cost,x)\n",
    "\n",
    "start = tf.constant([0.6, 0.8])  # Starting point for the search.\n",
    "optim_results = tfp.optimizer.bfgs_minimize(\n",
    "  quadratic_loss_and_gradient, initial_position=start, tolerance=1e-8)\n",
    "\n",
    "# Check that the search converged\n",
    "assert(optim_results.converged)\n",
    "# Check that the argmin is close to the actual value.\n",
    "np.testing.assert_allclose(optim_results.position, minimum)\n",
    "# Print out the total number of function evaluations it took. Should be 5.\n",
    "print (\"Function evaluations: %d\" % optim_results.num_objective_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2263107-ca16-4da3-8886-f2a20bf65743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
